# Research Evidence Base
## Single Source of Truth for Effect Sizes and Citations
## Version 1.0 | December 2025

---

## Purpose

This document consolidates all research evidence, effect sizes, and citations used across the KAMS Science curriculum. All framework documents should reference this file rather than duplicating effect sizes.

**Reference this document from:**
- `README.md` (AI instruction block)
- `framework/pedagogical-approach.md`
- `framework/CONTENT-DESIGN-GUIDE.md`
- `framework/mtss-framework.md`

---

## Quick Reference: Key Effect Sizes

| Strategy | Effect Size | Source | Implementation |
|----------|-------------|--------|----------------|
| Interleaving vs. blocked practice | d = 0.83 | Rohrer et al., 2020 | 2 spiral questions per exit ticket |
| High-information feedback | d = 0.99 | Wisniewski et al., 2020 | Refutational text structure |
| Active learning gap reduction (exams) | 33% | Theobald et al., 2020 | >67% active learning time |
| Active learning gap reduction (passing) | 45% | Theobald et al., 2020 | Station-based model |
| Virtual labs | g = 0.686 | 2024 PLOS ONE | PhET/HTML5 simulations |
| Retrieval practice vs. restudy | g = 0.50-0.61 | Rowland, 2014; Adesope, 2017 | Exit ticket spiral questions |
| Refutational text | g = 0.41 | Schroeder & Kucera, 2022 | Misconception feedback structure |
| UDL (school-aged) | g = 0.48 | King-Sears et al., 2023 | Tiered supports, multiple means |
| Digital science notebooks | γ = 0.34 | Rappolt-Schlichtmann et al., 2013 | Digital-first policy |
| Worked examples (science) | d = 0.70 | Barbieri et al., 2023 | Annotated exemplars |
| Sustained scaffolding | g = 0.46 | Belland et al., 2017 | Backward fading approach |
| Tiered instruction (science) | d = 1.06 | Richards & Omdal, 2007 | MTSS framework |
| Formative assessment | d = 0.40-0.70 | Black & Wiliam, 1998 | Embedded station checks |
| Metacognition instruction | d = 0.65-0.87 | Greene et al., 2018 | Confidence scales |
| Audio + visuals (modality) | d = 0.72 | Ginns, 2005 | Video narration design |
| Light guidance (simulations) | 85% exploration | Chamberlain et al., 2014 | Driving questions, not steps |

---

## 1. Retrieval Practice and Spacing

### Core Findings

**Interleaved vs. Blocked Practice**
- **Effect size: d = 0.83** (Rohrer et al., 2020)
- Study: RCT with 787 7th graders
- Finding: Mixed problem types produce significantly better retention than blocked practice

**Retrieval Practice vs. Restudy**
- **Effect size: g = 0.50-0.61** (Rowland, 2014; Adesope et al., 2017)
- Meta-analysis of testing effect literature
- Finding: Active retrieval strengthens memory more than passive review

**Optimal Spacing Interval**
- **10-20% of desired retention interval** (Cepeda et al., 2008)
- For semester retention (4-5 months): revisit every 2-3 weeks
- Finding: Spacing gaps should match retention goals

**Overt vs. Covert Retrieval**
- Overt retrieval (writing) > covert (mental) for adolescents (2025 study)
- Finding: Written responses produce stronger learning than mental retrieval

### Curriculum Implementation
- Exit tickets include 2 spiral questions from previous cycles
- Integration questions require both old and new concepts
- Target success rate: 50-80% (Yang et al., 2021)

### References
- Rohrer, D., Dedrick, R. F., Hartwig, M. K., & Cheung, C. N. (2020). Interleaved practice for mathematics. *Journal of Educational Psychology, 112*, 649-662.
- Rowland, C. A. (2014). The effect of testing versus restudy. *Psychonomic Bulletin & Review, 21*, 637-644.
- Adesope, O. O., et al. (2017). Rethinking the use of tests. *Review of Educational Research, 87*, 659-701.
- Cepeda, N. J., et al. (2008). Spacing effects in learning. *Psychological Science, 19*, 1095-1102.
- Roediger, H. L., & Karpicke, J. D. (2006). Test-enhanced learning. *Psychological Science, 17*, 249-255.

---

## 2. Feedback and Formative Assessment

### Core Findings

**High-Information Feedback**
- **Effect size: d = 0.99** (Wisniewski, Zierer, & Hattie, 2020)
- Finding: Information density matters more than timing alone
- Key: Feedback must address "Where am I going? How am I going? Where to next?"

**Formative Assessment Interventions**
- **Effect size: d = 0.40-0.70** (Black & Wiliam, 1998)
- Landmark review of 250+ studies
- Among largest effect sizes ever reported for educational interventions

**Feedback Type Comparison**
| Type | Effect Size | Source |
|------|-------------|--------|
| High-information feedback | d = 0.99 | Wisniewski et al., 2020 |
| Teacher feedback | d = 0.87 | Graham et al., 2015 |
| Student self-assessment | d = 0.61 | Lee et al., 2020 |
| Computer/automated feedback | d = 0.38-0.42 | Multiple meta-analyses |
| Reinforcement/punishment only | d = 0.24 | Wisniewski et al., 2020 |

**Metacognitive Instruction**
- **Effect sizes: d = 0.65-0.87** (Greene et al., 2018)
- RCT with 8th graders
- d = 0.65 for judgment accuracy, d = 0.87 for task value, d = 0.64 for performance

### Curriculum Implementation
- Refutational text structure for incorrect responses
- Confidence scales (0-point) for MTSS data
- Hattie & Timperley (2007) feedback framework

### References
- Black, P., & Wiliam, D. (1998). Inside the black box. *Phi Delta Kappan, 80*, 139-148.
- Wisniewski, B., Zierer, K., & Hattie, J. (2020). The power of feedback revisited. *Frontiers in Psychology, 10*, 3087.
- Hattie, J., & Timperley, H. (2007). The power of feedback. *Review of Educational Research, 77*, 81-112.
- Lee, H., et al. (2020). Self-assessment meta-analysis. *Educational Research Review, 30*, 100329.
- Greene, J. A., et al. (2018). Metacognitive instruction in science classrooms. *Journal of Research in Science Teaching, 55*, 1017-1043.
- Graham, S., et al. (2015). Teacher feedback in secondary writing. *Educational Psychology Review, 27*, 51-78.

---

## 3. Active Learning and Equity

### Core Findings

**Achievement Gap Reduction**
- **Exam score gap reduction: 33%** (Theobald et al., 2020 PNAS)
- **Passing rate gap reduction: 45%** (Theobald et al., 2020 PNAS)
- **High-intensity (>67%): 42% exam, 76% passing** (Theobald et al., 2020)
- Meta-analysis: 9,238 students (exams), 44,606 students (failure rates)

**Autonomy Support**
- **Correlation: r = 0.32** (meta-analysis of 153 studies, N = 213,612)
- Finding: Autonomy support correlates with learning outcomes

### Curriculum Implementation
- Station-based model targets >67% active learning time
- Student choice in expression formats
- Real-world phenomena connections

### References
- Theobald, E. J., et al. (2020). Active learning narrows achievement gaps. *PNAS, 117*, 6476-6483.
- Eccles, J. S., et al. (1993). Stage-environment fit. *American Psychologist, 48*, 90-101.
- Hulleman, C. S., & Harackiewicz, J. M. (2009). Making science relevant. *Science, 326*, 1410-1412.

---

## 4. Misconception Remediation

### Core Findings

**Refutational Text**
- **Effect size: g = 0.41** (Schroeder & Kucera, 2022; Danielson et al., 2025)
- Structure: State misconception → Refute → Correct explanation → Connect to evidence

**Bridging Analogies**
- Clement (1993): Connect anchoring intuitions to target through intermediate cases
- Example: Tables exert force → springs → flexible boards → rigid surfaces

**Two-Tier Diagnostic Tests**
- Treagust (1986, 1988): Content + reasoning tiers identify specific misconceptions

### Theoretical Frameworks

| Framework | Theorist | Key Insight |
|-----------|----------|-------------|
| Ontological Categories | Chi (1992, 2005) | Students miscategorize processes as matter |
| Knowledge-in-Pieces | diSessa | P-prims misapplied across contexts |
| Framework Theory | Vosniadou (2017) | Synthetic models from incompatible assimilation |

### Curriculum Implementation
- Distractor design based on documented misconceptions (Gierl et al., 2017)
- Refutational feedback on incorrect responses
- Spiral tracking of misconceptions across cycles

### References
- Chi, M. T. H. (2008). Three types of conceptual change. *Handbook of Research on Conceptual Change*, 61-82.
- diSessa, A. A. (1993). Toward an epistemology of physics. *Cognition and Instruction, 10*, 105-225.
- Vosniadou, S. (2017). Reading to learn science may create new misconceptions. *Learning and Instruction, 49*, 124-135.
- Schroeder, N. L., & Kucera, A. C. (2022). Refutational text meta-analysis. *Educational Psychology Review, 34*, 893-924.
- Clement, J. (1993). Using bridging analogies and anchoring intuitions. *Journal of Research in Science Teaching, 30*, 1241-1257.
- Treagust, D. F. (1988). Development and use of diagnostic tests. *International Journal of Science Education, 10*, 159-169.

---

## 5. Cognitive Load Theory

### Core Findings

**Working Memory Limit**
- **4-7 elements simultaneously** when processing novel information (Sweller, 1988)

**Worked Examples**
- **Effect size: d = 0.70** (Barbieri et al., 2023)
- Finding: Model complete reasoning before problem-solving

**Integrated Diagrams**
- **22% higher scores** (Chandler & Sweller, 1991)
- Finding: Embed labels in visuals; never separate text from images

**Modality Effect**
- **Effect size: d = 0.72** (Ginns, 2005)
- Finding: Audio + visuals > text + visuals

**Sustained Scaffolding**
- **Effect size: g = 0.46** (Belland et al., 2017)
- Finding: Scaffolding should be maintained longer than expected

**Backward Fading**
- Renkl & Atkinson (2003): Remove support from END of procedures first

### Curriculum Implementation
- Chunked content (10-15 minute segments)
- Worked examples with annotation
- Video uses narration, not on-screen text

### References
- Sweller, J. (1988). Cognitive load during problem solving. *Cognitive Science, 12*, 257-285.
- Barbieri, C. A., et al. (2023). Meta-analysis of worked examples in science and mathematics. *Educational Psychology Review.*
- Chandler, P., & Sweller, J. (1991). Cognitive load theory and the format of instruction. *Cognition and Instruction, 8*, 293-332.
- Ginns, P. (2005). Meta-analysis of the modality effect. *Learning and Instruction, 15*, 313-331.
- Renkl, A., & Atkinson, R. K. (2003). Structuring the transition from example study to problem solving. *Journal of Experimental Education, 70*, 293-315.
- Belland, B. R., et al. (2017). Meta-analysis of computer-based scaffolding in STEM. *Review of Educational Research, 87*, 309-344.

---

## 6. Interactive Simulations

### Core Findings

**Virtual Lab Effectiveness**
- **Effect size: g = 0.686** (2024 PLOS ONE meta-analysis)
- **Motivation: g = 3.571** (2024 PLOS ONE)
- **Engagement: g = 2.888** (2024 PLOS ONE)

**PhET vs. Real Equipment**
- Students using PhET **outperformed** real equipment users (Finkelstein et al., 2005)
- Benefits: conceptual understanding, practical application, system explanation

**Guidance Level**
- **Light guidance: 85% feature exploration**, better 1-week retention (Chamberlain et al., 2014)
- Heavy guidance: Only 9% feature exploration

### Curriculum Implementation
- Driving questions, not step-by-step instructions
- 5-10 minutes unguided exploration before tasks
- Prediction-observation-explanation sequences

### References
- Finkelstein, N. D., et al. (2005). PhET versus real equipment. *Physical Review Special Topics - Physics Education Research, 1*, 010103.
- Chamberlain, J. M., et al. (2014). Light guidance in simulations. *The Physics Teacher, 52*, 107-110.
- Wieman, C. E. (2020). *Improving How Universities Teach Science.* Harvard University Press.

---

## 7. Universal Design for Learning

### Core Findings

**UDL Effectiveness**
- **Overall: g = 0.43** (King-Sears et al., 2023)
- **School-aged learners: g = 0.48** (King-Sears et al., 2023)
- First methodologically rigorous UDL meta-analysis

**Digital Science Notebooks**
- **Effect size: γ = 0.34** learning improvement regardless of reading/writing proficiency (Rappolt-Schlichtmann et al., 2013)

### Three Principles (CAST UDL Guidelines 3.0)

| Principle | Brain Network | Implementation |
|-----------|---------------|----------------|
| Engagement | Affective | Choice, real-world phenomena, self-assessment |
| Representation | Recognition | Text + visuals + video, vocabulary support |
| Action/Expression | Strategic | Multiple response formats |

### Curriculum Implementation
- Digital-first materials policy
- Tiered supports via collapsible `<details>` elements
- Multiple means of expression

### References
- King-Sears, M. E., et al. (2023). UDL meta-analysis. *Exceptional Children, 89*, 361-384.
- Rappolt-Schlichtmann, G., et al. (2013). UDL web-based science notebooks. *Learning Disability Quarterly, 36*, 93-111.
- CAST. (2024). Universal Design for Learning Guidelines version 3.0.

---

## 8. MTSS and Tiered Intervention

### Core Findings

**Tiered Science Instruction**
- **Effect size: d = 1.06** (large) for low-background learners (Richards & Omdal, 2007)
- Effectively closes achievement gaps in secondary science

**Science CBM Limitations**
- Vocabulary-matching shows most promise (Conoyer et al., 2022)
- Reliability varies (.21-.89) vs. reading (.78-.99)
- Critical gap: No validated science-specific progress monitoring tools

**Data-Based Individualization**
- **Effect size: g = 0.57** for teacher outcomes (National Center on Intensive Intervention)

### Tier Parameters (Research-Based)

| Tier | Duration | Frequency | Group Size | Progress Monitoring |
|------|----------|-----------|------------|---------------------|
| 2 | 20-30 min | 3-4x weekly | 3-5 students | 2x monthly |
| 3 | Individualized | Weekly min | 1-3 (ideally 1:1) | Weekly |

### References
- Richards, S. B., & Omdal, S. N. (2007). Effects of tiered instruction on science achievement. *Remedial and Special Education, 28*, 227-241.
- Conoyer, S. J., et al. (2022). Science curriculum-based measures meta-analysis. *Journal of School Psychology, 93*, 1-22.
- National Center on Intensive Intervention. (2023). *Data-Based Individualization framework.*

---

## 9. NGSS and Three-Dimensional Learning

### Core Findings

**Phenomenon-Based Curriculum**
- Reiser et al. (2021): Anchoring phenomena must drive authentic questioning
- Lowell, Cherbow, & McNeill (2021): Many curricula "relabel" rather than "redesign"

**CCCs as Equity Entry Points**
- CCCs leverage students' funds of knowledge (Nordine & Lee, 2021)
- Students already use CCC-like thinking in everyday reasoning (Lee, Miller, & Januszyk, 2014)

### References
- NGSS Lead States. (2013). *Next Generation Science Standards.* Washington, DC: National Academies Press.
- Reiser, B. J., et al. (2021). Storyline-driven curriculum design. *Journal of Research in Science Teaching, 58*, 975-1006.
- Krajcik, J., et al. (2023). Multiple literacies in project-based learning. *Journal of Research in Science Teaching, 60*, 1803-1838.
- Nordine, J., & Lee, O. (2021). *Crosscutting Concepts.* NSTA Press.
- Lee, O., Miller, E., & Januszyk, R. (2014). CCCs for diverse learners. *Journal of Research in Science Teaching, 51*, 754-779.

---

## 10. ELL and Language Supports

### Core Findings

**Language Demands of NGSS**
- Science practices are inherently language-intensive (Lee, Quinn, & Valdés, 2013)
- Constructing explanations, arguing from evidence require academic language

**Five-Domain Framework** (Lee & Buxton)
1. Literacy strategies (all students)
2. Language support (ELLs)
3. Discourse strategies
4. Home language supports
5. Home culture connections

### Curriculum Implementation
- Pre-teach 5-7 key vocabulary terms with visuals
- Sentence stems for argumentation
- Bilingual glossary (Spanish)

### References
- Lee, O., Quinn, H., & Valdés, G. (2013). Language demands of NGSS. *Educational Researcher, 42*, 223-233.
- Lee, O., & Buxton, C. A. (2013). Teacher professional development for ELLs in science. *Review of Educational Research, 83*, 101-143.

---

## Primary Source Document

All research evidence is synthesized from:

**Scholarly Foundations for NGSS-Aligned Middle School Science Curriculum Development**

This comprehensive research synthesis provides the authoritative evidence base for all curriculum decisions. Effect sizes, citations, and implementation recommendations in this document are derived from that source.

---

*Research Evidence Base | Version 1.0 | December 2025*
*Single Source of Truth for KAMS Science Curriculum Research*
