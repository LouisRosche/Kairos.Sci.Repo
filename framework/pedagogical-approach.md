# KAMS Science Pedagogical Framework: "Weaponizing the Gaps"
## Evidence-Based Instruction Aligned with Scholarly Foundations
## Version 2.0 | December 2025

> **Research Reference:** For consolidated effect sizes and citations, see [docs/research-evidence.md](../docs/research-evidence.md) - the single source of truth for all research evidence.

---

## Executive Summary

This curriculum implements **evidence-based instructional strategies** synthesized from cognitive science, misconception research, and NGSS-aligned three-dimensional learning. The research base converges on powerful principles: retrieval practice with interleaving produces effect sizes of **d = 0.83** (Rohrer et al., 2020), active learning reduces achievement gaps by **33-45%** (Theobald et al., 2020), and phenomenon-based three-dimensional learning drives authentic sensemaking.

Rather than explicit re-teaching, our curriculum creates contexts where:

1. Incorrect mental models generate visible cognitive dissonance
2. Successful task completion requires retrieval of prior concepts
3. Transfer is assessed through novel phenomena, not repeated prompts
4. High-intensity active learning (>67% of class time) maximizes equity outcomes

**Core Research Foundation:**

| Principle | Effect Size | Source |
|-----------|-------------|--------|
| Interleaved vs. blocked practice | d = 0.83 | Rohrer et al., 2020 |
| Retrieval practice vs. restudy | g = 0.50-0.61 | Rowland, 2014; Adesope, 2017 |
| Active learning gap reduction (exam scores) | 33% | Theobald et al., 2020 |
| Active learning gap reduction (passing rates) | 45% | Theobald et al., 2020 |
| High-information feedback | d = 0.99 | Wisniewski et al., 2020 |
| Metacognition/self-regulation | +7 months progress | EEF, 2023 |
| Worked examples in science | d = 0.70 | Barbieri et al., 2023 |

---

## Materials Policy: Digital-First Approach

> **CRITICAL REQUIREMENT:** All instructional materials must be digital-hosted in this repository. The ONLY permitted physical material is **one notecard per student per class period** for note-taking or quick reference.

**Rationale:**
- Digital science notebooks produce **γ = 0.34** learning improvement regardless of reading/writing proficiency (Rappolt-Schlichtmann et al., 2013)
- Ensures 100% asynchronous accessibility for absent/remote learners
- Supports UDL principles with text-to-speech, adjustable text size, and embedded supports
- Enables data-driven MTSS monitoring through digital assessment

**Notecard Guidelines:**
- Students may use one 3×5 or 4×6 notecard per class period
- Notecard content is student-generated (not pre-printed)
- Notecards support retrieval practice through self-testing
- Notecards may be collected for formative assessment insights

---

## Cognitive Load Theory: Foundational Design Principles

John Sweller's cognitive load theory establishes that working memory can process only **4-7 elements** simultaneously when encountering novel information. This constraint fundamentally shapes how science content is organized in this curriculum.

### Three Types of Load

| Load Type | Definition | Curriculum Response |
|-----------|------------|---------------------|
| **Intrinsic** | Inherent complexity of content | Sequence from low to high element interactivity |
| **Extraneous** | Poor instructional design | Integrated diagrams, chunked content, progressive disclosure |
| **Germane** | Schema construction | Worked examples, self-explanation prompts, backward fading |

### Evidence-Based Cognitive Load Strategies

1. **Worked Examples (d = 0.70)**: Model complete scientific reasoning processes before problem-solving
2. **Integrated Diagrams (22% higher scores)**: Embed labels directly within scientific visuals—never separate text from referenced images
3. **Backward Fading**: Remove scaffolding from the END of procedures first, not the beginning (Renkl & Atkinson, 2003)
4. **Chunked Content**: 10-15 minute learning segments maximum before breaks (aligns with Pomodoro integration)
5. **Sustained Scaffolding (g = 0.46)**: Contrary to assumptions, scaffolding should be maintained longer than expected (Belland et al., 2017)

### Implementation: Modality Principle

- Audio narration combined with visuals produces **d = 0.72** versus text with visuals (Ginns, 2005)
- Videos and simulations should use voiceover, not on-screen text
- Avoid redundant text that repeats what is said aloud

---

## 3-Dimensional Learning Framework

### Science and Engineering Practices (SEP)
Each station targets specific practices aligned with NGSS:
- **SEP 1:** Asking Questions and Defining Problems
- **SEP 4:** Analyzing and Interpreting Data
- **SEP 6:** Constructing Explanations and Designing Solutions
- **SEP 7:** Engaging in Argument from Evidence

### Disciplinary Core Ideas (DCI)
Grade-specific progressions building on prior cycles while introducing new content. See `standards-alignment.md` for complete mapping.

### Crosscutting Concepts (CCC) as Equity Entry Points
Research establishes CCCs as "complementary lenses on phenomena" that can leverage students' funds of knowledge from homes and communities (Nordine & Lee, 2021). CCCs serve as entry points for historically marginalized students because students already use CCC-like thinking in everyday reasoning (Lee, Miller, & Januszyk, 2014).

- **Cause and Effect:** Mechanism-based reasoning
- **Energy and Matter:** Conservation and flow through systems
- **Stability and Change:** Equilibrium and feedback loops
- **Patterns:** Recognition and prediction
- **Scale and Proportion:** Relative quantities and relationships
- **Systems and Models:** Interconnected components
- **Structure and Function:** Form determines purpose

---

## Station-Based Learning Model

> **Note for Remote/Async Learners:** The term "station" refers to **timed learning segments** that scholars complete sequentially, NOT physical locations. Whether in-class or at home, scholars work through each station in order on their Chromebook. All materials are embedded in the student page — no physical setup required for remote learners.

### Structure
Each week follows a consistent station rotation:
1. **Hook (Day 1):** Phenomenon-driven engagement, prior knowledge activation
2. **Station 1 (Day 1):** Core concept with interleaved retrieval
3. **Station 2 (Day 1):** Application with physical manipulatives
4. **Station 3 (Day 2):** Engineering design challenge
5. **Exit Ticket (Day 3):** Mixed assessment (new + spiral + integration)

### Timing
- Long periods (65 min): Hook + 2 stations + break
- Short periods (45 min): Single station focus
- Exit periods (65 min): Assessment + catch-up/extension

### Pomodoro Integration
- 25-minute focused station work
- 5-minute movement/social break between stations
- Clear visual timer
- Break activities: stretch, quick walk, drink water

---

## Assessment Architecture: Evidence-Based Formative Assessment

### Research Foundation

Black and Wiliam's (1998) landmark review of 250+ studies found effect sizes of **0.40-0.70** for formative assessment interventions—among the largest ever reported for educational interventions. Critically, feedback effectiveness depends on **information density, not timing alone** (Wisniewski, Zierer, & Hattie, 2020).

| Feedback Type | Effect Size | Source |
|--------------|-------------|--------|
| High-information feedback | d = 0.99 | Wisniewski et al., 2020 |
| Student self-assessment | d = 0.61 | Lee et al., 2020 |
| Teacher feedback | d = 0.87 | Graham et al., 2015 |
| Computer/automated feedback | d = 0.38-0.42 | Multiple meta-analyses |
| Reinforcement/punishment only | d = 0.24 | Wisniewski et al., 2020 |

### The Hattie & Timperley (2007) Feedback Framework

Effective feedback addresses three questions:
1. **Where am I going?** (Clear goals/learning targets)
2. **How am I going?** (Current performance relative to goals)
3. **Where to next?** (Actionable next steps for improvement)

### Formative Assessment (Embedded in Stations)
Each station includes:
1. **Entry checkpoint:** Quick retrieval question from previous cycle (targets "How am I going?")
2. **Process monitoring:** Teacher observation protocol with checklist
3. **Exit checkpoint:** Application question combining old + new concepts

### Metacognition Integration (d = 0.65-0.87)

Greene et al.'s (2018) RCT with 8th graders demonstrated metacognitive instruction produces:
- **d = 0.65** for metacognitive judgment accuracy
- **d = 0.87** for task value motivation
- **d = 0.64** for conceptual performance

**Implementation:** Confidence scales and self-assessment prompts are embedded in every exit ticket (0-point to avoid grade inflation but critical for MTSS data).

### Exit Ticket Structure (Every Week)
| Component | Count | Purpose |
|-----------|-------|---------|
| NEW content | 2 | Current cycle learning targets |
| SPIRAL content | 2 | Retrieval practice (10-20% of retention interval) |
| INTEGRATION | 1 | Requires both current and prior cycles |
| SEP-1 Question Generator | 1 | Student-generated HOW/WHY questions |

**Format:** Google Forms → Auto-sync to Canvas
**Target Success Rate:** 50-80% (Yang et al., 2021)—if too easy, increase difficulty; if too hard, add scaffolding

### Point Distribution (Per Week = 100 pts)
| Component | Points | Grading Type |
|-----------|--------|--------------|
| Hook | 12 | Mixed (auto + manual) |
| Station 1 | 20 | Mixed |
| Station 2 | 20 | Mixed |
| Station 3 | 25 | Mostly manual (design) |
| Exit Ticket | 23 | Mixed |

---

## Misconception-Targeted Design: Research-Based Framework

### Three Theoretical Frameworks for Understanding Misconceptions

| Framework | Theorist | Key Insight | Curriculum Application |
|-----------|----------|-------------|------------------------|
| **Ontological Categories** | Chi (1992, 2005, 2008) | Students miscategorize processes as matter (e.g., heat as a substance) | Explicitly address category shifts; more difficult than within-category corrections |
| **Knowledge-in-Pieces** | diSessa | P-prims like "closer is stronger" work in many contexts but are misapplied | Help students recognize when each intuition applies appropriately |
| **Framework Theory** | Vosniadou (2017) | "Synthetic models" form when new info assimilates into incompatible frameworks | Identify and explicitly address prior frameworks before introducing new concepts |

### Evidence-Based Misconception Remediation Strategies

1. **Refutational Text (g = 0.41)**: Structure follows: state misconception → directly refute → provide correct explanation → connect to evidence (Schroeder & Kucera, 2022; Danielson et al., 2025)

2. **Bridging Analogies** (Clement, 1993): Connect anchoring intuitions to target concepts through intermediate bridging cases
   - Example: Tables DO exert upward force → build from springs (accepted) → flexible boards → rigid surfaces

3. **Two-Tier Diagnostic Tests** (Treagust, 1986, 1988): Combine content questions with reasoning tiers to identify specific misconception types

### Philosophy: "Weaponizing the Gaps"
Rather than re-teaching prior cycle content directly, we:
1. Design tasks where misconceptions cause visible failures
2. Create "misconception traps" with explicit distractors grounded in documented student conceptions (Gierl et al., 2017)
3. Use immediate, high-information feedback to trigger cognitive revision
4. Spiral key concepts in novel contexts for discrimination practice

### High-Frequency Misconceptions by Discipline

**Earth Science:**
- Summer caused by Earth being closer to Sun (closer-is-stronger p-prim)
- Water evaporates and "disappears" (matter misconception)
- Groundwater flows in underground rivers (false visualization)

**Life Science:**
- Plants get food from soil (nutrition misconception)
- Energy is recycled in ecosystems (energy-matter conflation)
- Evolution is goal-directed (teleological reasoning)

**Physical Science:**
- Objects need force to keep moving (impetus misconception)
- Heat is a substance that flows (ontological category error)
- Current is "used up" by bulbs (sequential reasoning)

### Implementation
- **Distractor Analysis:** Each MCQ includes misconceptions as options based on documented research
- **Explicit Labeling:** Questions tagged as (NEW), (SPIRAL), or (INTEGRATION)
- **Feedback Design:** Incorrect responses get refutational text explaining WHY the answer is wrong
- **Rubric Precision:** Observable behaviors, not subjective judgments
- **Spiral Tracking:** Same misconceptions revisited across cycles in different contexts

---

## Differentiation Framework: UDL 3.0 and Equity-Centered Design

### Research Foundation: Universal Design for Learning

CAST's UDL Guidelines 3.0 (July 2024) organize 35 considerations under three principles aligned with brain networks. King-Sears et al.'s (2023) meta-analysis—the first methodologically rigorous UDL meta-analysis—found **g = 0.43** overall, with school-aged learners showing stronger effects (**g = 0.48**).

| UDL Principle | Brain Network | Curriculum Implementation |
|---------------|---------------|---------------------------|
| **Engagement** | Affective | Choice in expression, real-world phenomena, self-assessment |
| **Representation** | Recognition | Text + visuals + video, vocabulary support, worked examples |
| **Action/Expression** | Strategic | Typed + drawn + recorded responses, varied question types |

### Universal Design Features (All Materials)
- Chunked instructions (numbered steps, one action per line) — respects 4-7 element working memory limit
- Visual supports for all abstract concepts
- Multiple means of expression (verbal, written, drawn, recorded)
- Extended time built into asynchronous model
- Digital-first design enables text-to-speech, adjustable fonts, and embedded supports

### Tiered Accommodations

| Accommodation | Implementation |
|--------------|----------------|
| Extended time | Async model allows unlimited time; no time-based penalties |
| Reduced questions | Mark "required" questions; others optional |
| Text-to-speech | Google Forms compatible with screen readers |
| Simplified language | Vocabulary glossary QR code (accessible on student page) |
| Graphic organizers | Blank cause-effect diagrams, calculation templates |
| Frequent breaks | Pomodoro structure (25 on/5 off) |
| Calculator use | Permitted for all calculation questions |
| Word bank | Provided for constructed responses |

### English Learner Supports (Okhee Lee & Cary Buxton Framework)

Research establishes that NGSS science practices are inherently language-intensive—constructing explanations, engaging in argument, and communicating information all require academic language proficiency (Lee, Quinn, & Valdés, 2013).

**Five-Domain ELL Framework:**
1. **Literacy Strategies** (all students): Visual-heavy phenomena, chunked text
2. **Language Support** (ELLs): Pre-teach 5-7 key vocabulary terms per lesson with visual supports
3. **Discourse Strategies**: Sentence stems scaffolding scientific argumentation
4. **Home Language Supports**: Bilingual glossary (Spanish available), word-to-word translation permitted
5. **Home Culture Connections**: Phenomena connect to diverse students' communities

**Specific Supports:**
- Bilingual glossary (Spanish available)
- Sentence frames: "The evidence shows... because...", "Based on the pattern, I predict... because..."
- Partner reading protocol for dense text
- Visual-heavy phenomena (videos, simulations over text)
- Academic language functions explicitly taught (describe, explain, predict, infer, conclude)

### Extension Opportunities
- Challenge problems marked with star
- "Design your own phenomenon" option
- Peer teaching opportunities
- Connection to current research articles

---

## Motivation Research: Preventing Middle School Science Disengagement

### Stage-Environment Fit Theory (Eccles et al., 1993)

Middle school environments systematically mismatch adolescent developmental needs:
- Larger class sizes reduce personalized attention
- Greater teacher control contradicts increasing autonomy needs
- Emphasis on grades intensifies social comparison during identity formation

**Curriculum Response:** Station-based learning increases autonomy and reduces whole-class comparison.

### Self-Determination Theory: Three Basic Needs

| Need | Curriculum Design Response |
|------|---------------------------|
| **Autonomy** | Student choice in investigation topics, methods, presentation formats |
| **Competence** | Scaffolded complexity, formative feedback emphasizing growth |
| **Relatedness** | Collaborative learning, peer communities, teacher mentorship |

Research shows autonomy support produces **r = 0.32** correlation with learning outcomes (meta-analysis of 153 studies, N = 213,612).

### Utility Value Interventions

Hulleman and Harackiewicz's (2009) *Science* publication found having students write about how science topics relate to their lives enhanced course grades and subsequent interest for **low-performing students** specifically.

**Implementation:** Every station includes real-world application connections; Exit Ticket SEP-1 questions invite student-generated connections.

### Interest Development (Hidi & Renninger, 2006)

| Phase | Curriculum Support |
|-------|-------------------|
| 1. Triggered situational interest | Surprising phenomena, demonstrations |
| 2. Maintained situational interest | Ongoing novelty, collaborative tasks |
| 3. Emerging individual interest | Student-selected investigations |
| 4. Well-developed individual interest | Independent inquiry pathways |

**Key insight:** Frame learning as "doing science" rather than "being scientists"—self-efficacy declines with age for identity framing (β = -0.13, p = .046) but not for action framing (β = -0.02, p = .74).

---

## Equity Integration: Active Learning as Achievement Gap Intervention

### Research Foundation (Theobald et al., 2020 PNAS)

Meta-analysis of 9,238 students (exam scores) and 44,606 students (failure rates):

| Active Learning Intensity | Exam Score Gap Reduction | Passing Rate Gap Reduction |
|---------------------------|--------------------------|---------------------------|
| Any active learning | 33% | 45% |
| High-intensity (>67% class time) | **42%** | **76%** |

**Implication:** Our station-based model targets >67% active learning time to maximize equity outcomes.

### Culturally Responsive Pedagogy (Ladson-Billings)

Three integrated components:
1. **Academic Success:** Demonstrable growth for all students
2. **Cultural Competence:** Grounding in one's culture while acquiring fluency in others
3. **Critical Consciousness:** Using school knowledge to address real-world problems

### Asset-Based Approaches

**Funds of Knowledge (Moll et al.):** Household knowledge as curricular resource—phenomena connect to students' homes and communities.

**Community Cultural Wealth (Yosso):** Six forms of capital students bring:
- Aspirational, Linguistic, Familial, Social, Navigational, Resistant

### Science Identity Development (Carlone & Johnson, 2007)

Three dimensions: competence (knowledge), performance (demonstration), recognition (being seen as a "science person").

**Key implementation:** Provide multiple ways to demonstrate competence; ensure all students receive recognition for scientific thinking.

---

## Recovery Path Design

### Asynchronous Completion Options
Students who miss live instruction can:
1. Access video walkthroughs of each station (linked in Canvas)
2. Complete station tasks independently on Chromebook
3. Submit at any point during the cycle (forgiving grading policy)

### Just-in-Time Support
- Each station includes "Stuck? Start Here" section
- QR codes link to 2-minute concept refreshers
- Peer pairing protocol for collaborative support

---

## Teacher Facilitation Guidelines

### During Stations
- Circulate with clipboard using observation protocol
- Ask probing questions when misconceptions appear
- Flag students for 1:1 follow-up if needed
- Use "strategic pausing" to address whole-class patterns

### Misconception Response Protocol
1. **Identify:** Notice incorrect reasoning in student work
2. **Probe:** Ask clarifying question (don't immediately correct)
3. **Redirect:** Point to evidence/simulation that contradicts misconception
4. **Confirm:** Have student articulate revised understanding

### Chromebook Management
- All materials accessible via Canvas modules
- Offline-capable Google Forms
- Station rotation timer displayed on board
- Emergency paper backups available

---

## Technical Integration

### Platform Stack
- **LMS:** Canvas (modules, assignments, gradebook)
- **Assessment:** Google Forms (embedded via LTI)
- **Data Hub:** Google Sheets (IMPORTRANGE formulas)
- **Simulations:** PhET and custom HTML5 (browser-based, Chromebook-friendly)

### Data Flow
1. Students submit via Google Forms
2. Responses auto-populate linked spreadsheet
3. Hub aggregates across forms via IMPORTRANGE
4. Scores sync to Canvas gradebook

---

## Interactive Simulations: Evidence-Based Guidance

### Research Foundation

Carl Wieman's PhET project, informed by over 200 student think-aloud interviews, established design principles for educational simulations. Finkelstein et al.'s (2005) landmark study found students using PhET **outperformed** those using real equipment on conceptual understanding, practical application, and explaining how systems work.

| Finding | Effect Size | Source |
|---------|-------------|--------|
| Virtual vs. physical labs (conceptual) | Near zero difference | 2023 Frontiers in Education |
| Virtual labs overall | g = 0.686 | 2024 PLOS ONE |
| Learning motivation (virtual) | g = 3.571 | 2024 PLOS ONE |
| Engagement (virtual) | g = 2.888 | 2024 PLOS ONE |

### Light Guidance Produces Deeper Exploration

Chamberlain et al.'s (2014) critical finding:
- **Light guidance:** 85% feature exploration, better retention at 1 week
- **Heavy guidance:** Only 9% feature exploration

**Implication:** Use driving questions rather than step-by-step instructions for simulations.

### Design Principles for Productive Simulation Use

1. **Allow 5-10 minutes of unguided exploration** before formal tasks
2. **Use driving questions** rather than step-by-step instructions
3. **Embed prediction-observation-explanation sequences**
4. **Connect simulation experiences** to real-world phenomena when possible
5. **Include reflection prompts** linking observations to underlying concepts

### PhET Implicit Scaffolding Framework

Effective simulations use affordances, constraints, cueing, and feedback to guide without students feeling directed—preserving autonomy critical for engagement.

---

## References

### Cognitive Load Theory
- Sweller, J. (1988). Cognitive load during problem solving. *Cognitive Science, 12*, 257-285.
- Barbieri, C. A., et al. (2023). Meta-analysis of worked examples in science and mathematics. *Educational Psychology Review.*
- Chandler, P., & Sweller, J. (1991). Cognitive load theory and the format of instruction. *Cognition and Instruction, 8*, 293-332.
- Ginns, P. (2005). Meta-analysis of the modality effect. *Learning and Instruction, 15*, 313-331.
- Renkl, A., & Atkinson, R. K. (2003). Structuring the transition from example study to problem solving. *Journal of Experimental Education, 70*, 293-315.
- Belland, B. R., et al. (2017). Meta-analysis of computer-based scaffolding in STEM. *Review of Educational Research, 87*, 309-344.

### Misconception Research
- Chi, M. T. H. (2008). Three types of conceptual change. *Handbook of Research on Conceptual Change*, 61-82.
- diSessa, A. A. (1993). Toward an epistemology of physics. *Cognition and Instruction, 10*, 105-225.
- Vosniadou, S. (2017). Reading to learn science may create new misconceptions. *Learning and Instruction, 49*, 124-135.
- Schroeder, N. L., & Kucera, A. C. (2022). Refutational text meta-analysis. *Educational Psychology Review, 34*, 893-924.
- Clement, J. (1993). Using bridging analogies and anchoring intuitions. *Journal of Research in Science Teaching, 30*, 1241-1257.
- Treagust, D. F. (1988). Development and use of diagnostic tests. *International Journal of Science Education, 10*, 159-169.

### Formative Assessment
- Black, P., & Wiliam, D. (1998). Inside the black box. *Phi Delta Kappan, 80*, 139-148.
- Wisniewski, B., Zierer, K., & Hattie, J. (2020). The power of feedback revisited. *Frontiers in Psychology, 10*, 3087.
- Hattie, J., & Timperley, H. (2007). The power of feedback. *Review of Educational Research, 77*, 81-112.
- Lee, H., et al. (2020). Self-assessment meta-analysis. *Educational Research Review, 30*, 100329.
- Greene, J. A., et al. (2018). Metacognitive instruction in science classrooms. *Journal of Research in Science Teaching, 55*, 1017-1043.

### Retrieval Practice and Spacing
- Rohrer, D., Dedrick, R. F., Hartwig, M. K., & Cheung, C. N. (2020). Interleaved practice for mathematics. *Journal of Educational Psychology, 112*, 649-662.
- Rowland, C. A. (2014). The effect of testing versus restudy. *Psychonomic Bulletin & Review, 21*, 637-644.
- Adesope, O. O., et al. (2017). Rethinking the use of tests. *Review of Educational Research, 87*, 659-701.
- Cepeda, N. J., et al. (2008). Spacing effects in learning. *Psychological Science, 19*, 1095-1102.
- Roediger, H. L., & Karpicke, J. D. (2006). Test-enhanced learning. *Psychological Science, 17*, 249-255.

### Interactive Simulations
- Finkelstein, N. D., et al. (2005). PhET versus real equipment. *Physical Review Special Topics - Physics Education Research, 1*, 010103.
- Chamberlain, J. M., et al. (2014). Light guidance in simulations. *The Physics Teacher, 52*, 107-110.
- Wieman, C. E. (2020). *Improving How Universities Teach Science.* Harvard University Press.

### NGSS and Three-Dimensional Learning
- NGSS Lead States. (2013). *Next Generation Science Standards.* Washington, DC: National Academies Press.
- Reiser, B. J., et al. (2021). Storyline-driven curriculum design. *Journal of Research in Science Teaching, 58*, 975-1006.
- Krajcik, J., et al. (2023). Multiple literacies in project-based learning. *Journal of Research in Science Teaching, 60*, 1803-1838.
- Nordine, J., & Lee, O. (2021). *Crosscutting Concepts.* NSTA Press.
- Lee, O., Miller, E., & Januszyk, R. (2014). CCCs for diverse learners. *Journal of Research in Science Teaching, 51*, 754-779.

### Motivation and Engagement
- Eccles, J. S., et al. (1993). Stage-environment fit. *American Psychologist, 48*, 90-101.
- Hidi, S., & Renninger, K. A. (2006). The four-phase model of interest development. *Educational Psychologist, 41*, 111-127.
- Hulleman, C. S., & Harackiewicz, J. M. (2009). Making science relevant. *Science, 326*, 1410-1412.
- Carlone, H. B., & Johnson, A. (2007). Understanding the science experiences of successful women of color. *Journal of Research in Science Teaching, 44*, 1187-1218.

### Equity and UDL
- Theobald, E. J., et al. (2020). Active learning narrows achievement gaps. *PNAS, 117*, 6476-6483.
- King-Sears, M. E., et al. (2023). UDL meta-analysis. *Exceptional Children, 89*, 361-384.
- Rappolt-Schlichtmann, G., et al. (2013). UDL web-based science notebooks. *Learning Disability Quarterly, 36*, 93-111.
- Moll, L. C., et al. (1992). Funds of knowledge. *Theory Into Practice, 31*, 132-141.
- Yosso, T. J. (2005). Community cultural wealth. *Race Ethnicity and Education, 8*, 69-91.
- Lee, O., Quinn, H., & Valdés, G. (2013). Language demands of NGSS. *Educational Researcher, 42*, 223-233.

### MTSS and Intervention
- Richards, S. B., & Omdal, S. N. (2007). Effects of tiered instruction on science achievement. *Remedial and Special Education, 28*, 227-241.
- Conoyer, S. J., et al. (2022). Science curriculum-based measures meta-analysis. *Journal of School Psychology, 93*, 1-22.

---

*Framework Version 2.0 | December 2025 | KAMS Science*
*Aligned with Scholarly Foundations for NGSS-Aligned Middle School Science Curriculum Development*
